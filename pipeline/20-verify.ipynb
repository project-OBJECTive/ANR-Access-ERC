{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some NLP rules on each properties to see if it makes sense given the lot description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import pandas as pd\n",
    "import lib\n",
    "import yaml\n",
    "import ollama\n",
    "\n",
    "# Paremeters from config file\n",
    "with open(\"./00-config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "catalog = config['catalog']['folder_name']\n",
    "local_model = config['model']['local_model']\n",
    "cooldown = config['model']['local_cooldown']\n",
    "\n",
    "# Overwrite variables in case of pipeline mode\n",
    "if os.getenv('OBJECTIVE_MODE') == 'pipeline':\n",
    "    catalog = os.getenv('OBJECTIVE_CATALOG')\n",
    "    \n",
    "# Global variables\n",
    "folder_path = f\"../catalogs/{catalog}\"\n",
    "eta = lib.Eta()\n",
    "input_path = f\"{folder_path}/objects.csv\"\n",
    "output_path = f\"{folder_path}/objects.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = pd.read_csv(input_path)\n",
    "objects['index'] = objects['index'].astype(pd.StringDtype())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and validate strange records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the flag for each lines\n",
    "columns_before = list(objects.columns)\n",
    "objects['verify'] = None\n",
    "objects = objects[['verify'] + columns_before]\n",
    "\n",
    "eta.begin(len(objects), \"Find and flag strange records\")\n",
    "for i, row in objects.iterrows():\n",
    "\n",
    "    verify = []\n",
    "\n",
    "    # Verify presence of index in the description\n",
    "    index = str(row['index']).replace('.0', '')\n",
    "    if index not in row['description'] or index.strip() == '':\n",
    "        verify.append('index')\n",
    "\n",
    "    # Verify presence of each object_type in description\n",
    "    if pd.notna(row['object_type']):\n",
    "        object_types = row['object_type'].lower().split(', ')\n",
    "        for object_type in object_types:\n",
    "            # Remove trailing \"s\"\n",
    "            if object_type.endswith('s') or object_type.endswith('x'): \n",
    "                object_type = object_type[:-1]\n",
    "            if object_type not in row['description'].lower() and 'object_type' not in verify:\n",
    "                verify.append('object_type')\n",
    "        if object_types == '':\n",
    "            verify.append('object_type')\n",
    "\n",
    "    # Ollama verifications - Object Type\n",
    "    if \"object_type\" in verify:\n",
    "        object_types = row['object_type'].lower().split(', ')\n",
    "        for object_type in object_types:\n",
    "            prompt = f\"From the following object description, can we say that the main object is a {object_type}?\\nHere is the description: \\\"{row['description']}\\\"\\nAnswer with a single word: \\\"yes\\\" or \\\"no\\\", with no additionnal explaination.\"\n",
    "            # # Temp\n",
    "            # print('')\n",
    "            # print('>>> OLLAMA PROMPT')\n",
    "            # print(prompt)\n",
    "            messages = [{ \"role\": \"user\", \"content\": prompt }]\n",
    "            response = ollama.chat(model=local_model, messages=messages) \n",
    "            time.sleep(cooldown) # To let computer cool down\n",
    "            answer: str = response['message']['content']\n",
    "            # # Temp\n",
    "            # print('')\n",
    "            # print('>>> OLLAMA ANSWER')\n",
    "            # print(answer)\n",
    "            # print('---')\n",
    "\n",
    "            # If LLM says yes, save result\n",
    "            if \"yes\" in answer.lower():\n",
    "                try: verify.remove(\"object_type\")\n",
    "                except: pass\n",
    "\n",
    "\n",
    "    # Verify presence of each material in description\n",
    "    if pd.notna(row['material_technique']):\n",
    "        materials = row['material_technique'].lower().split(', ')\n",
    "        for material in materials:\n",
    "            # Remove trailing \"s\"\n",
    "            if material.endswith('s') or material.endswith('x'): \n",
    "                material = material[:-1]\n",
    "            if material not in row['description'].lower() and 'material_technique' not in verify:\n",
    "                verify.append('material_technique')\n",
    "\n",
    "    # Ollama verifications - Material & Techniques\n",
    "    if \"material_technique\" in verify:\n",
    "        material_techniques = row['material_technique'].lower().split(', ')\n",
    "        for material_technique in material_techniques:\n",
    "            prompt = f\"From the following object description, can we say that the main object is made of {material_technique}?\\nHere is the description: \\\"{row['description']}\\\"\\nAnswer with a single word: \\\"yes\\\" or \\\"no\\\", with no additionnal explaination.\"\n",
    "            # # Temp\n",
    "            # print('')\n",
    "            # print('>>> OLLAMA PROMPT')\n",
    "            # print(prompt)\n",
    "            messages = [{ \"role\": \"user\", \"content\": prompt }]\n",
    "            response = ollama.chat(model=local_model, messages=messages) \n",
    "            time.sleep(cooldown) # To let computer cool down\n",
    "            answer: str = response['message']['content']\n",
    "            # # Temp\n",
    "            # print('')\n",
    "            # print('>>> OLLAMA ANSWER')\n",
    "            # print(answer)\n",
    "            # print('---')\n",
    "\n",
    "            # If LLM says yes, save result\n",
    "            if \"yes\" in answer.lower():\n",
    "                try: verify.remove(\"material_technique\")\n",
    "                except: pass\n",
    "\n",
    "\n",
    "    # Verify presence of each origin in description\n",
    "    if pd.notna(row['origin']):\n",
    "        origins = row['origin'].lower().split(', ')\n",
    "        for origin in origins:\n",
    "            # Remove trailing \"s\"\n",
    "            if origin.endswith('s') or origin.endswith('x'): \n",
    "                origin = origin[:-1]\n",
    "            if origin not in row['description'].lower() and 'origin' not in verify:\n",
    "                verify.append('origin')\n",
    "\n",
    "    # Ollama verifications - Origin\n",
    "    if \"origin\" in verify:\n",
    "        origins = row['origin'].lower().split(', ')\n",
    "        for origin in origins:\n",
    "            prompt = f\"From the following object description, can we say that the main object comes from {origin}?\\nHere is the description: \\\"{row['description']}\\\"\\nAnswer with a single word: \\\"yes\\\" or \\\"no\\\", with no additionnal explaination.\"\n",
    "            # # Temp\n",
    "            # print('')\n",
    "            # print('>>> OLLAMA PROMPT')\n",
    "            # print(prompt)\n",
    "            messages = [{ \"role\": \"user\", \"content\": prompt }]\n",
    "            response = ollama.chat(model=local_model, messages=messages) \n",
    "            time.sleep(cooldown) # To let computer cool down\n",
    "            answer: str = response['message']['content']\n",
    "            # # Temp\n",
    "            # print('')\n",
    "            # print('>>> OLLAMA ANSWER')\n",
    "            # print(answer)\n",
    "            # print('---')\n",
    "\n",
    "            # If LLM says yes, save result\n",
    "            if \"yes\" in answer.lower():\n",
    "                try: verify.remove(\"origin\")\n",
    "                except: pass\n",
    "\n",
    "    \n",
    "    # Set the verification flag\n",
    "    objects.at[i, 'verify'] = ', '.join(verify)\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_verify = objects[objects['verify'] != '']\n",
    "print('Verify count:', len(to_verify), f\"({lib.percent(len(to_verify) / len(objects))})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save objects with verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
