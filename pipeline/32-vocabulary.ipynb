{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list of objects and their information, extract the vocabulary of each property into one single CSV table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, requests, time\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import pandas as pd\n",
    "import lib\n",
    "import yaml\n",
    "\n",
    "# Paremeters from config file\n",
    "with open(\"./00-config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "spacy_model = config['catalog']['spacy_model']\n",
    "\n",
    "# Global Variables\n",
    "eta = lib.Eta()\n",
    "input_path = f'../data/objects-all.csv'\n",
    "output_path = f\"../data/vocabulary-all.csv\"\n",
    "vocabulary = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load information table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get LOD mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(output_path):\n",
    "    \n",
    "    # Extract already mapped vocabulary\n",
    "    existing_vocab = pd.read_csv(output_path)\n",
    "    existing_vocab = existing_vocab[pd.notna(existing_vocab['identifier'])]\n",
    "\n",
    "    # Create a key (index)\n",
    "    existing_vocab['key'] = existing_vocab['type'] + '-' + existing_vocab['name']\n",
    "    existing_vocab.set_index('key', inplace=True)\n",
    "\n",
    "    # Make the existing mapped vocabulary useable\n",
    "    lod = existing_vocab.to_dict(orient='index')\n",
    "\n",
    "else:\n",
    "    lod = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get object type vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words\n",
    "raw_object_types = list(table['object_type'].dropna())\n",
    "\n",
    "# Extract each one of them in case there are multiples in a single cell\n",
    "all_object_types = ', '.join(list(map(lambda x: str(x), raw_object_types))).split(', ')\n",
    "\n",
    "# Make them unique (should already be the case)\n",
    "all_object_types = list(set(all_object_types))\n",
    "\n",
    "# Get the cardinality of each one of them (LOT cardinality)\n",
    "eta.begin(len(all_object_types), 'Counting object types')\n",
    "for object_type in all_object_types:\n",
    "    count = sum([lib.has_element(obj_types, object_type) for obj_types in table['object_type'].tolist()])\n",
    "    vocabulary.append({\n",
    "        \"type\": \"object_type\",\n",
    "        \"name\": object_type,\n",
    "        \"count\": count\n",
    "    })\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get materials and techniques vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words\n",
    "raw_material_techniques = list(table['material_technique'].dropna())\n",
    "\n",
    "# Extract each one of them in case there are multiples in a single cell\n",
    "all_material_techniques = ', '.join(list(map(lambda x: str(x), raw_material_techniques))).split(', ')\n",
    "\n",
    "# Make them unique\n",
    "all_material_techniques = list(set(all_material_techniques))\n",
    "\n",
    "# Get the cardinality of each one of them (LOT cardinality)\n",
    "eta.begin(len(all_material_techniques), 'Counting materials and techniques')\n",
    "for material_technique in all_material_techniques:\n",
    "    count = sum([lib.has_element(mat_techs, material_technique.strip()) for mat_techs in table['material_technique'].tolist()])\n",
    "    vocabulary.append({\n",
    "        \"type\": \"material_technique\",\n",
    "        \"name\": material_technique,\n",
    "        \"count\": count\n",
    "    })\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get origins vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words\n",
    "raw_origins = list(table['origin'].dropna())\n",
    "\n",
    "# Extract each one of them in case there are multiples in a single cell\n",
    "all_origins = ', '.join(list(map(lambda x: str(x), raw_origins))).split(', ')\n",
    "\n",
    "# Make them unique\n",
    "all_origins = list(set(all_origins))\n",
    "\n",
    "# Get the cardinality of each one of them (LOT cardinality)\n",
    "eta.begin(len(all_origins), 'Counting origins')\n",
    "for origin in all_origins:\n",
    "    count = sum([lib.has_element(orgs, origin.strip()) for orgs in table['origin'].tolist()])\n",
    "    vocabulary.append({\n",
    "        \"type\": \"origin\",\n",
    "        \"name\": origin,\n",
    "        \"count\": count\n",
    "    })\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get author vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words\n",
    "raw_authors = list(table['author'].dropna())\n",
    "\n",
    "# Extract each one of them in case there are multiples in a single cell\n",
    "all_authors = ', '.join(list(map(lambda x: str(x), raw_authors))).split(', ')\n",
    "\n",
    "# Make them unique\n",
    "all_authors = list(set(all_authors))\n",
    "\n",
    "# Get the cardinality of each one of them (LOT cardinality)\n",
    "eta.begin(len(all_authors), 'Counting authors')\n",
    "for author in all_authors:\n",
    "    count = sum([lib.has_element(orgs, author.strip()) for orgs in table['author'].tolist()])\n",
    "    vocabulary.append({\n",
    "        \"type\": \"author\",\n",
    "        \"name\": author,\n",
    "        \"count\": count\n",
    "    })\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Period vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words\n",
    "raw_periods = list(table['period'].dropna())\n",
    "\n",
    "# Extract each one of them in case there are multiples in a single cell\n",
    "all_periods = ', '.join(list(map(lambda x: str(x), raw_periods))).split(', ')\n",
    "\n",
    "# Make them unique\n",
    "all_periods = list(set(all_periods))\n",
    "\n",
    "# Get the cardinality of each one of them (LOT cardinality)\n",
    "eta.begin(len(all_periods), 'Counting periods')\n",
    "for period in all_periods:\n",
    "    count = sum([lib.has_element(orgs, period.strip()) for orgs in table['period'].tolist()])\n",
    "    vocabulary.append({\n",
    "        \"type\": \"period\",\n",
    "        \"name\": period,\n",
    "        \"count\": count\n",
    "    })\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map LODs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve information\n",
    "for i, vocab in enumerate(vocabulary):\n",
    "    key = vocab['type'] + '-' + vocab['name']\n",
    "    if key in lod:\n",
    "        vocabulary[i]['authority_file'] = lod[key]['authority_file']\n",
    "        vocabulary[i]['identifier'] = lod[key]['identifier']\n",
    "        if lod[key]['label']: vocabulary[i]['label'] = lod[key]['label']\n",
    "        if lod[key]['definition']: vocabulary[i]['definition'] = lod[key]['definition']\n",
    "        if lod[key]['category']: vocabulary[i]['category'] = lod[key]['category']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill LOD informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_info(identifier):\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?label ?definition WHERE {{\n",
    "      wd:{identifier.strip()} rdfs:label ?label ;\n",
    "                     schema:description ?definition .\n",
    "      FILTER (lang(?label) = \"en\" && lang(?definition) = \"en\")\n",
    "    }}\n",
    "    \"\"\"\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    response = requests.get(endpoint_url, params={\"query\": query}, headers=headers)\n",
    "    time.sleep(1)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()['results']['bindings']\n",
    "        if results:\n",
    "            return {\n",
    "                \"label\": results[0]['label']['value'],\n",
    "                \"definition\": results[0]['definition']['value']\n",
    "            }\n",
    "        else:\n",
    "            return {\"label\": None, \"definition\": None}\n",
    "    else:\n",
    "        print(query)\n",
    "        raise Exception(f\"SPARQL query failed with status {response.status_code}\")\n",
    "    \n",
    "\n",
    "def get_getty_info(identifier):\n",
    "    endpoint_url = \"https://vocab.getty.edu/sparql\"\n",
    "    concept_uri = f\"<http://vocab.getty.edu/aat/{identifier.strip()}>\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        select \n",
    "            (COALESCE(?label_, \"\") as ?label)\n",
    "            (COALESCE(?note_, \"\") as ?definition)\n",
    "        where {{\n",
    "            optional {{\n",
    "                {concept_uri} xl:prefLabel ?label_obj .\n",
    "                ?label_obj gvp:term ?label_ .\n",
    "                FILTER(lang(?label_) = \"en\")\n",
    "            }}\n",
    "            optional {{\n",
    "                {concept_uri} skos:scopeNote ?note_obj .\n",
    "                ?note_obj rdf:value ?note_ .\n",
    "                FILTER(lang(?note_) = \"en\")\n",
    "            }}\n",
    "        }}\n",
    "    \"\"\"\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    response = requests.get(endpoint_url, params={\"query\": query}, headers=headers)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()['results']['bindings']\n",
    "        if results:\n",
    "            return {\n",
    "                \"label\": results[0]['label']['value'],\n",
    "                \"definition\": results[0]['definition']['value']\n",
    "            }\n",
    "        else:\n",
    "            return {\"label\": None, \"definition\": None}\n",
    "    else:\n",
    "        print(query)\n",
    "        raise Exception(f\"SPARQL query failed with status {response.status_code}\")\n",
    "    \n",
    "\n",
    "def get_getty_root_type(uri):\n",
    "    endpoint_url = \"https://vocab.getty.edu/sparql\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        select \n",
    "            *\n",
    "            # ?broader ?label\n",
    "        where {{\n",
    "            {uri} <http://vocab.getty.edu/ontology#broader> ?broader .\n",
    "            ?broader xl:prefLabel ?label_obj .\n",
    "            ?label_obj gvp:term ?label .\n",
    "            FILTER(lang(?label) = \"en\" || lang(?label) = \"en-US\" || lang(?label) = \"en-us\")\n",
    "        }}\n",
    "    \"\"\"\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    response = requests.get(endpoint_url, params={\"query\": query}, headers=headers)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()['results']['bindings']\n",
    "        def parse(element):\n",
    "            to_return = {}\n",
    "            for key, value in element.items():\n",
    "                to_return[key] = value['value']\n",
    "            return to_return\n",
    "        return pd.DataFrame(data=[parse(elt) for elt in results])\n",
    "    else:\n",
    "        print(query)\n",
    "        raise Exception(f\"SPARQL query failed with status {response.status_code}\")\n",
    "    \n",
    "\n",
    "def get_getty_categ(uri):\n",
    "    while True:\n",
    "        try:\n",
    "            result = get_getty_root_type(f\"<{uri.strip()}>\")\n",
    "            if 'http://vocab.getty.edu/aat/300010357' in result['broader'].tolist(): return \"material\"\n",
    "            elif 'http://vocab.getty.edu/aat/300053001' in result['broader'].tolist(): return \"technique\"\n",
    "            else: uri = result.iloc[0]['broader']\n",
    "        except:\n",
    "            return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.begin(len(vocabulary), 'Finding LOD informations')\n",
    "for i, vocab in enumerate(vocabulary):\n",
    "    label_missing = 'label' not in vocab or pd.isna(vocab['label']) or vocab['label'].strip() == ''\n",
    "    definition_missing = 'definition' not in vocab or pd.isna(vocab['definition']) or vocab['definition'].strip() == ''\n",
    "    categ_missing = 'category' not in vocab or pd.isna(vocab['category']) or vocab['category'].strip() == ''\n",
    "\n",
    "    if 'authority_file' in vocab and label_missing and definition_missing:\n",
    "        eta.print(f'Fetching information for {vocab[\"type\"]}/{vocab[\"name\"]}')   \n",
    "        if vocab['authority_file'] == 'wikidata': infos = get_wikidata_info(vocab['identifier'])\n",
    "        elif vocab['authority_file'] == 'getty': infos = get_getty_info(vocab['identifier'])\n",
    "        else: raise Exception(f'Unknown Authority file \"{vocab[\"authority_file\"]}\" for word <{vocab[\"type\"]}/{vocab[\"name\"]}>')\n",
    "        vocabulary[i]['label'] = infos['label'].capitalize() if infos['label'] else ''\n",
    "        vocabulary[i]['definition'] = infos['definition'].capitalize() if infos['definition'] else ''\n",
    "\n",
    "    if vocab['type'] == 'material_technique' and categ_missing and 'identifier' in vocab and vocab['authority_file'] == 'getty':\n",
    "        uri = f\"http://vocab.getty.edu/aat/{vocab['identifier']}\"\n",
    "        eta.print(f'Fetching category for {vocab[\"type\"]}/{vocab[\"name\"]}')   \n",
    "        vocabulary[i]['category'] = get_getty_categ(uri)\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and save catalog vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and save\n",
    "vocabulary = pd.DataFrame(data=vocabulary)\n",
    "vocabulary['count'].astype(pd.Int64Dtype())\n",
    "vocabulary.sort_values('count', ascending=False, inplace=True)\n",
    "vocabulary.to_csv(output_path, index=False)\n",
    "\n",
    "# Also, filter a dataframe for each dedicated vocabulary\n",
    "vocabulary_object_type = vocabulary[vocabulary['type'] == 'object_type']\n",
    "vocabulary_material_technique = vocabulary[vocabulary['type'] == 'material_technique']\n",
    "vocabulary_origin = vocabulary[vocabulary['type'] == 'origin']\n",
    "vocabulary_author = vocabulary[vocabulary['type'] == 'author']\n",
    "vocabulary_period = vocabulary[vocabulary['type'] == 'period']\n",
    "\n",
    "# Save all\n",
    "vocabulary_object_type.to_csv(output_path.replace('all', 'object-type'), index=False)\n",
    "vocabulary_material_technique.to_csv(output_path.replace('all', 'material-technique'), index=False)\n",
    "vocabulary_origin.to_csv(output_path.replace('all', 'origin'), index=False)\n",
    "vocabulary_author.to_csv(output_path.replace('all', 'author'), index=False)\n",
    "vocabulary_period.to_csv(output_path.replace('all', 'period'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
