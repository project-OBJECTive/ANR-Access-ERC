{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform extracted text into their lemma version. \n",
    "\n",
    "First step of harmonization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import warnings\n",
    "import lib\n",
    "import yaml\n",
    "\n",
    "# Paremeters from config file\n",
    "with open(\"./00-config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "catalog = config['catalog']['folder_name']\n",
    "spacy_model = config['catalog']['spacy_model']\n",
    "\n",
    "# Overwrite variables in case of pipeline mode\n",
    "if os.getenv('OBJECTIVE_MODE') == 'pipeline':\n",
    "    catalog = os.getenv('OBJECTIVE_CATALOG')\n",
    "    \n",
    "# Global Variables\n",
    "eta = lib.Eta()\n",
    "folder_path = f\"../catalogs/{catalog}\"\n",
    "input_path = f'{folder_path}/objects.csv'\n",
    "output_path = f'{folder_path}/objects.csv'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nlp = spacy.load(spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get objects types lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.begin(len(objects), \"Get lemma of object types\")\n",
    "for i, row in objects.iterrows():\n",
    "    if pd.isna(row['object_type']): continue\n",
    "    object_types = row['object_type'].split(', ')\n",
    "    new_object_types = []\n",
    "    for object_type in object_types:\n",
    "        doc = nlp(object_type)\n",
    "        lemmas = \"\"\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'NOUN': \n",
    "                lemmas += token.lemma_ + ' '\n",
    "            else: \n",
    "                lemmas += token.text + ' '\n",
    "        new_object_types.append(lemmas.strip())\n",
    "\n",
    "    objects.at[i, 'object_type'] = ', '.join(new_object_types)\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get materials & techniques lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.begin(len(objects), \"Get lemma of materials and techniques\")\n",
    "for i, row in objects.iterrows():\n",
    "    if pd.isna(row['material_technique']): continue\n",
    "    mats_techs = row['material_technique'].split(', ')\n",
    "    new_mats_techs = []\n",
    "    for mats_techs in mats_techs:\n",
    "        doc = nlp(mats_techs)\n",
    "        lemmas = \"\"\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'NOUN': \n",
    "                lemmas += token.lemma_ + ' '\n",
    "            else: \n",
    "                lemmas += token.text + ' '\n",
    "        new_mats_techs.append(lemmas.strip())\n",
    "\n",
    "    objects.at[i, 'material_technique'] = ', '.join(new_mats_techs)\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get origins lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta.begin(len(objects), \"Get lemma of origins\")\n",
    "for i, row in objects.iterrows():\n",
    "    if pd.isna(row['origin']): continue\n",
    "    origins = row['origin'].split(', ')\n",
    "    new_origin = []\n",
    "    for origin in origins:\n",
    "        doc = nlp(origin)\n",
    "        lemmas = \"\"\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'NOUN': \n",
    "                lemmas += token.lemma_ + ' '\n",
    "            else: \n",
    "                lemmas += token.text + ' '\n",
    "        new_origin.append(lemmas.strip())\n",
    "\n",
    "    objects.at[i, 'origin'] = ', '.join(new_origin)\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
