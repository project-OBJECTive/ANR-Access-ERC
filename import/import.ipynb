{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bb0269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 16:31:35\n"
     ]
    }
   ],
   "source": [
    "# Begin time\n",
    "from datetime import datetime\n",
    "begin = datetime.now()\n",
    "print(\"Started at:\", begin.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a951418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and global constants\n",
    "\n",
    "import os, pandas as pd\n",
    "import unicodedata\n",
    "from utils import generate_id, Eta, to_literal\n",
    "from utils2 import populate_entity\n",
    "from dotenv import load_dotenv\n",
    "from sparql import SPARQL\n",
    "from sparql_graphdb import GraphDB\n",
    "from prefix import prefixes, Prefix\n",
    "from ontology import classes as c, properties as p, constants as k, Classes, Properties\n",
    "load_dotenv()\n",
    "eta = Eta()\n",
    "\n",
    "root_uri = 'http://geovistory.org/resource/'\n",
    "data_named_graph = ''\n",
    "shacl_named_graph = 'http://geovistory.org/resource/shacl'\n",
    "\n",
    "# Preparation\n",
    "prefixes.append(Prefix('root', root_uri))\n",
    "\n",
    "# Set up the SPARQL wrapper\n",
    "sparql: SPARQL = GraphDB(os.getenv('URL'), os.getenv('USERNAME'), os.getenv('PASSWORD'))\n",
    "sparql.prefixes = prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a4da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uri(identifier: str = None):\n",
    "    if identifier is not None: \n",
    "        return f\"root:i{identifier}\"\n",
    "    else:\n",
    "        return f\"root:i{generate_id()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702460f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e978f",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48fa6373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Joining Soltykoff prices and buyers - 1069 iterations in 00h00m10s (105.0 iter/sec)                      \n",
      "[ETA] Joining San Donato prices - 1707 iterations in 00h00m38s (44.1 iter/sec)                                 \n"
     ]
    }
   ],
   "source": [
    "# Read data (output of pipeline)\n",
    "\n",
    "normalize = lambda x: unicodedata.normalize(\"NFC\", x)\n",
    "\n",
    "vocabulary = pd.read_csv('../data/vocabulary-all.csv')\n",
    "catalogs = pd.read_csv('../data/catalogs.csv', sep=\";\", converters={'name': normalize})\n",
    "lots = pd.read_csv('../data/objects-all.csv', converters={'catalog': normalize})\n",
    "sales_san_donato = pd.read_csv('../data/sales-san-donato.csv')\n",
    "sales_soltykoff = pd.read_csv('../data/sales-soltykoff.csv')\n",
    "\n",
    "# Split the vocabulary\n",
    "\n",
    "origins = vocabulary[vocabulary['type'] == 'origin'].copy()\n",
    "origins.drop(columns=['type', 'category'], inplace=True)\n",
    "\n",
    "material_techniques = vocabulary[vocabulary['type'] == 'material_technique'].copy()\n",
    "material_techniques.drop(columns=['type'], inplace=True)\n",
    "\n",
    "periods = pd.read_csv('../data/vocabulary-period.csv') # Here we read it directly from disk, because it has additional information about begin year and end year\n",
    "periods.drop(columns=['type', 'category'], inplace=True)\n",
    "periods['begin_year'] = periods['begin_year'].astype(pd.Int64Dtype())\n",
    "periods['end_year'] = periods['end_year'].astype(pd.Int64Dtype())\n",
    "\n",
    "object_types = vocabulary[vocabulary['type'] == 'object_type'].copy()\n",
    "object_types.drop(columns=['type', 'category'], inplace=True)\n",
    "\n",
    "authors = vocabulary[vocabulary['type'] == 'author'].copy()\n",
    "authors.drop(columns=['type', 'category'], inplace=True)\n",
    "\n",
    "# Join sale prices and buyers to lots\n",
    "\n",
    "# Joining Soltykoff prices and buyers\n",
    "selection = lots[lots['catalog'].str.contains('Soltykoff')]\n",
    "eta.begin(len(selection), \"Joining Soltykoff prices and buyers\")\n",
    "for i, lot in selection.iterrows():\n",
    "    for _, sale in sales_soltykoff.iterrows():\n",
    "        if lot['index'] == sale['index']:\n",
    "            lots.at[i, 'sale_price'] = sale['price']\n",
    "            lots.at[i, 'sale_currency_uri'] = k.currency_franc\n",
    "            lots.at[i, 'sale_currency_label'] = \"Francs\"\n",
    "        if pd.notna(sale['buyer']) and any(x in sale['index'].split(', ') for x in lot['index'].split(', ')):\n",
    "            lots.at[i, 'sale_buyer'] = sale['buyer']\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "# Joining San Donato prices\n",
    "selection = lots[lots['catalog'].str.contains('Donato')]\n",
    "eta.begin(len(selection), \"Joining San Donato prices\")\n",
    "for i, lot in selection.iterrows():\n",
    "    for _, sale in sales_san_donato.iterrows():\n",
    "        if lot['index'] == sale['index']:\n",
    "            lots.at[i, 'sale_price'] = sale['price']\n",
    "            lots.at[i, 'sale_currency_uri'] = k.currency_lires\n",
    "            lots.at[i, 'sale_currency_label'] = \"Lires\"\n",
    "    eta.iter()\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dec391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Endpoint\n",
    "sparql.run('CLEAR ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5d1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create constants\n",
    "\n",
    "triples = []\n",
    "\n",
    "# Language: French\n",
    "label = 'French'\n",
    "definition = 'French language'\n",
    "triples += populate_entity(k.language_french, c.language, label, definition, \"en\")\n",
    "\n",
    "# Entity Quality Type: Number\n",
    "label = 'Number'\n",
    "definition = 'Number of items in a set'\n",
    "triples += populate_entity(k.entityQualityType_number, c.entity_quality_type, label, definition, \"en\")\n",
    "\n",
    "# Francs\n",
    "label = \"Franc\"\n",
    "definition = \"Unité monétaire unique de la France qu'entre le 7 avril 1795 et le 31 décembre 1998.\"\n",
    "triples += populate_entity(k.currency_franc, c.currency, label, definition)\n",
    "\n",
    "# Lire italiennes\n",
    "label = \"Lire\"\n",
    "definition = \"Ancienne unité monétaire de l'Italie, émise du 17 mars 1861 au 28 février 2002.\"\n",
    "triples += populate_entity(k.currency_lires, c.currency, label, definition)\n",
    "\n",
    "# Lugt number\n",
    "label = \"Lugt number\"\n",
    "triples += populate_entity(k.identifierType_lugt, c.identifier_type, label)\n",
    "\n",
    "# URL\n",
    "label = \"URL\"\n",
    "triples += populate_entity(k.identifierType_url, c.identifier_type, label)\n",
    "\n",
    "# Residence\n",
    "label = \"Résidence\"\n",
    "triples += populate_entity(k.epistemicLocationType_residence, c.epistemic_location_type, label)\n",
    "\n",
    "# Residence\n",
    "label = \"Lieu de distribution\"\n",
    "triples += populate_entity(k.epistemicLocationType_distributionPlace, c.epistemic_location_type, label)\n",
    "\n",
    "# Page number\n",
    "label = \"Nombre de pages\"\n",
    "triples += populate_entity(k.entityQualityType_pageNumber, c.entity_quality_type, label)\n",
    "\n",
    "# Lot numbers\n",
    "label = \"Nombre de lots\"\n",
    "triples += populate_entity(k.entityQualityType_lotNumber, c.entity_quality_type, label)\n",
    "\n",
    "# Fees\n",
    "label = \"Frais de ventes\"\n",
    "triples += populate_entity(k.quantiQualSpati_fees, c.quantifiable_quality_of_a_spatio_temporal_phenomenon, label)\n",
    "\n",
    "# Auctioneer\n",
    "label = \"Commissaire-priseur\"\n",
    "triples += populate_entity(k.actorSocialQuality_auctioneer, c.actor_social_quality, label)\n",
    "\n",
    "# Expert\n",
    "label = \"Expert\"\n",
    "triples += populate_entity(k.actorSocialQuality_expert, c.actor_social_quality, label)\n",
    "\n",
    "# Seller\n",
    "label = \"Vendeur\"\n",
    "triples += populate_entity(k.actorSocialQuality_seller, c.actor_social_quality, label)\n",
    "\n",
    "# Auction\n",
    "label = \"Vente aux enchères\"\n",
    "triples += populate_entity(k.activityType_auction, c.activity_type, label)\n",
    "\n",
    "# Auction\n",
    "label = \"Prix d'adjudication\"\n",
    "triples += populate_entity(k.quantiQualIntEventType_hammerPrice, c.quantifiable_quality_of_an_intentional_event_type, label)\n",
    "\n",
    "# Buyer\n",
    "label = \"Acheteur\"\n",
    "triples += populate_entity(k.actorSocialQuality_buyer, c.actor_social_quality, label)\n",
    "\n",
    "\n",
    "# Insert\n",
    "sparql.insert(triples, data_named_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da261a40",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85e5902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating Geographical places - 174 iterations in 00h00m00s (727.2 iter/sec)                              \n",
      "[ETA] Inserting triples - 0.508 iterations in 00h00m00s (3.6 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Origins (Geographical places)\n",
    "\n",
    "triples = []\n",
    "eta.begin(len(origins), 'Creating Geographical places')\n",
    "for i, row in origins.iterrows():\n",
    "\n",
    "    # Create Instance\n",
    "    uri = create_uri()\n",
    "    label = row['name']\n",
    "    triples += populate_entity(uri, c.geographical_place, label.title())\n",
    "\n",
    "    # Add URI\n",
    "    if row['authority_file'] == 'wikidata': \n",
    "        same_as = \"http://www.wikidata.org/entity/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "    elif row['authority_file'] == 'getty': \n",
    "        same_as = \"https://vocab.getty.edu/aat/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "\n",
    "    # Save URI\n",
    "    origins.at[i, 'uri'] = uri\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()\n",
    "\n",
    "# To have them accessible with keys\n",
    "origins_dict = origins.set_index('name').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d430232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating Materials & Techniques - 2589 iterations in 00h00m03s (672.5 iter/sec)                          \n",
      "[ETA] Inserting triples - 5.338 iterations in 00h00m01s (4.9 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Materials and techniques\n",
    "\n",
    "triples = []\n",
    "eta.begin(len(material_techniques), 'Creating Materials & Techniques')\n",
    "for i, row in material_techniques.iterrows():\n",
    "\n",
    "    # Create Instance\n",
    "    uri = create_uri()\n",
    "    label = row['name']\n",
    "    triples += populate_entity(uri, c.general_technique, label.title())\n",
    "\n",
    "    # Add URI\n",
    "    if row['authority_file'] == 'wikidata' and row['identifier']: \n",
    "        same_as = \"http://www.wikidata.org/entity/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "    elif row['authority_file'] == 'getty' and row['identifier']:\n",
    "        same_as = \"https://vocab.getty.edu/aat/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "\n",
    "    # Save URI\n",
    "    material_techniques.at[i, 'uri'] = uri\n",
    "\n",
    "    # If we know it, add \"material\" or \"technique\" as a comment\n",
    "    if pd.notna(row['category']):\n",
    "        triples.append((uri, p.has_note, to_literal(row['category'])))\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()\n",
    "\n",
    "# To have them accessible with keys\n",
    "material_techniques_dict = material_techniques.set_index('name').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0095ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating Periods - 27 iterations in 00h00m00s (660.6 iter/sec)                                           \n",
      "[ETA] Inserting triples - 0.092 iterations in 00h00m00s (0.9 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Periods \n",
    "\n",
    "triples = []\n",
    "eta.begin(len(periods), 'Creating Periods')\n",
    "for i, row in periods.iterrows():\n",
    "\n",
    "    # Build label\n",
    "    label = row['name']\n",
    "    if \"siècle\" in label:\n",
    "        words = label.split(' ')\n",
    "        label = words[0][:-1].upper() + 'e siècle'\n",
    "\n",
    "    # Create Instance\n",
    "    uri = create_uri()\n",
    "    triples += populate_entity(uri, c.time_span, row['label'])\n",
    "\n",
    "    # Add URI\n",
    "    if row['authority_file'] == 'wikidata' and row['identifier']: \n",
    "        same_as = \"http://www.wikidata.org/entity/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "    elif row['authority_file'] == 'getty' and row['identifier']:\n",
    "        same_as = \"https://vocab.getty.edu/aat/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "\n",
    "    # Save URI\n",
    "    periods.at[i, 'uri'] = uri\n",
    "\n",
    "    # Add begin and end year\n",
    "    triples += [\n",
    "        (uri, p.begin_of_the_begin, to_literal(str(row['begin_year']))),\n",
    "        (uri, p.end_of_the_end, to_literal(str(row['end_year']))),\n",
    "    ]\n",
    "\n",
    "    # If approx\n",
    "    if row['approx'] == \"approx\":\n",
    "        triples += [(uri, p.has_note, to_literal(\"Approximatif\", \"fr\"))]\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()\n",
    "\n",
    "# To have them accessible with keys\n",
    "periods_dict = periods.set_index('name').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40efc501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating Object types - 1925 iterations in 00h00m02s (672.5 iter/sec)                                    \n",
      "[ETA] Inserting triples - 4.0 iterations in 00h00m00s (5.1 iter/sec)                                           \n"
     ]
    }
   ],
   "source": [
    "# Create object types \n",
    "\n",
    "triples = []\n",
    "eta.begin(len(object_types), 'Creating Object types')\n",
    "for i, row in object_types.iterrows():\n",
    "\n",
    "    # Create Instance\n",
    "    uri = create_uri()\n",
    "    label = row['name']\n",
    "    triples += populate_entity(uri, c.physical_human_made_thing_type, label.title())\n",
    "    \n",
    "    # Add URI\n",
    "    if row['authority_file'] == 'wikidata': \n",
    "        same_as = \"http://www.wikidata.org/entity/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "    elif row['authority_file'] == 'getty': \n",
    "        same_as = \"https://vocab.getty.edu/aat/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "\n",
    "    # Save URI\n",
    "    object_types.at[i, 'uri'] = uri\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()\n",
    "\n",
    "# To have them accessible with keys\n",
    "object_types_dict = object_types.set_index('name').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e8b9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating Authors - 319 iterations in 00h00m00s (721.5 iter/sec)                                          \n",
      "[ETA] Inserting triples - 0.638 iterations in 00h00m00s (4.2 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Create authors\n",
    "\n",
    "triples = []\n",
    "eta.begin(len(authors), 'Creating Authors')\n",
    "for i, row in authors.iterrows():\n",
    "\n",
    "    # Create Instance\n",
    "    uri = create_uri()\n",
    "    label = row['name']\n",
    "    triples += populate_entity(uri, c.actor, label.title())\n",
    "\n",
    "    # Add URI\n",
    "    if row['authority_file'] == 'wikidata': \n",
    "        same_as = \"http://www.wikidata.org/entity/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "    elif row['authority_file'] == 'getty': \n",
    "        same_as = \"https://vocab.getty.edu/aat/\" + row['identifier']\n",
    "        triples.append((uri, p.same_as, to_literal(same_as)))\n",
    "\n",
    "    # Save URI\n",
    "    authors.at[i, 'uri'] = uri\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()\n",
    "\n",
    "# To have them accessible with keys\n",
    "authors_dict = authors.set_index('name').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e43d9",
   "metadata": {},
   "source": [
    "# Core Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "864f7946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating Sale Places - 16 iterations in 00h00m00s (701.1 iter/sec)                                       \n",
      "[ETA] Inserting triples - 0.034 iterations in 00h00m00s (0.1 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Import occupations\n",
    "sale_places = catalogs[['sale_place']].dropna().drop_duplicates()\n",
    "\n",
    "triples = []\n",
    "saleplaces_uris = {}\n",
    "eta.begin(len(sale_places), 'Creating Sale Places')\n",
    "for i, row in sale_places.iterrows():\n",
    "    for sale_place in row['sale_place'].split('\\n'):\n",
    "        saleplace_uri = create_uri()\n",
    "        saleplace_label = sale_place\n",
    "        triples += populate_entity(saleplace_uri, c.geographical_place, saleplace_label)\n",
    "\n",
    "        saleplaces_uris[sale_place] = saleplace_uri\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e7d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare actors & addresses\n",
    "\n",
    "auctioneers = []\n",
    "experts = []\n",
    "sellers = []\n",
    "distributed_places = []\n",
    "for i, row in catalogs.iterrows():\n",
    "\n",
    "    # Auctioneer\n",
    "    if pd.notna(row['auctioneer_names']):\n",
    "        auctioneers_names = row['auctioneer_names'].split('\\n')\n",
    "        auctioneers_addresses = row['auctioneer_addresses'].split('\\n') if pd.notna(row['auctioneer_addresses']) else [''] * len(auctioneers_names)\n",
    "        auctioneers += [(name, address) for name, address in zip(auctioneers_names, auctioneers_addresses)]\n",
    "\n",
    "    # Expert\n",
    "    if pd.notna(row['experts_names']):\n",
    "        experts_names = row['experts_names'].split('\\n')\n",
    "        experts_addresses = row['expert_addresses'].split('\\n') if pd.notna(row['expert_addresses']) else [''] * len(experts_names)\n",
    "        experts += [(name, address) for name, address in zip(experts_names, experts_addresses)]\n",
    "\n",
    "    # Seller\n",
    "    if pd.notna(row['seller_name']):\n",
    "        sellers_names = row['seller_name'].split('\\n')\n",
    "        sellers_addresses = row['seller_address'].split('\\n') if pd.notna(row['seller_address']) else [''] * len(sellers_names)\n",
    "        sellers += [(name, address) for name, address in zip(sellers_names, sellers_addresses)]\n",
    "\n",
    "    # Distributed places and persons\n",
    "    if pd.notna(row['catalog_distributed_places']):\n",
    "        for distributed_place in row['catalog_distributed_places'].split('\\n'):\n",
    "            separator = ' - '\n",
    "            separator_index = distributed_place.rindex(separator)\n",
    "            if separator_index:\n",
    "                address = distributed_place[0:separator_index]\n",
    "                actor = distributed_place[separator_index + len(separator):]\n",
    "                distributed_places.append((actor, address))\n",
    "\n",
    "# Buyers\n",
    "buyers = []\n",
    "for buyer in lots['sale_buyer'].unique().tolist():\n",
    "    buyers.append((buyer, ''))\n",
    "\n",
    "\n",
    "# Regroup addresses of persons\n",
    "all_persons = {}\n",
    "for person, address in auctioneers + experts + sellers + buyers:\n",
    "    if person == '': continue\n",
    "    if person not in all_persons: all_persons[person] = [address]\n",
    "    else: all_persons[person].append(address)\n",
    "\n",
    "# Remove actors if they are known persons, and add addresses to persons\n",
    "actors_only = []\n",
    "for actor, address in distributed_places:\n",
    "    if actor == '': continue\n",
    "    if actor not in all_persons: actors_only.append((actor, address))\n",
    "    else: all_persons[actor].append(address)\n",
    "\n",
    "# Regroup addresses of actors\n",
    "all_actors = {}\n",
    "for actor, address in actors_only:\n",
    "    if actor == '': continue\n",
    "    if actor not in all_actors: all_actors[actor] = [address]\n",
    "    else: all_actors[actor].append(address)\n",
    "\n",
    "# Make addresses unique and fetch all addresses\n",
    "all_addresses = set()\n",
    "for key, value in all_persons.items():\n",
    "    all_persons[key] = list(set(value))\n",
    "    for address in value:\n",
    "        all_addresses.add(address)\n",
    "for key, value in all_actors.items():\n",
    "    all_actors[key] = list(set(value))\n",
    "    for address in value:\n",
    "        all_addresses.add(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23526db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Inserting triples - 0.216 iterations in 00h00m00s (2.0 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Import addresses\n",
    "\n",
    "triples = []\n",
    "eta.begin(len(all_addresses), 'Importing addresses')\n",
    "addresses_uris = {}\n",
    "for address in all_addresses:\n",
    "    if address == '': continue\n",
    "\n",
    "    address_uri = create_uri()\n",
    "    address_label = address\n",
    "    triples += populate_entity(address_uri, c.geographical_place, address_label)\n",
    "\n",
    "    addresses_uris[address_label] = address_uri\n",
    "    \n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ebe127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Inserting triples - 0.428 iterations in 00h00m00s (3.1 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Import actors with addresses\n",
    "\n",
    "triples = []\n",
    "actors_uris = {}\n",
    "eta.begin(len(all_actors), 'Importing actors')\n",
    "for actor, addresses in all_actors.items():\n",
    "\n",
    "    # Create actor\n",
    "    actor_uri = create_uri()\n",
    "    actor_label = actor\n",
    "    triples += populate_entity(actor_uri, c.actor, actor_label)\n",
    "\n",
    "    # Link to Geographical place\n",
    "    for address in addresses:\n",
    "        geoplace_uri = addresses_uris[address]\n",
    "\n",
    "        epistemic_location_uri = create_uri()\n",
    "        epistemic_location_label = f\"{address} - {actor_label}\"\n",
    "        triples += populate_entity(epistemic_location_uri, c.epistemic_location_of_a_physical_thing, epistemic_location_label)\n",
    "        triples += [\n",
    "            (epistemic_location_uri, p.is_localized_at, geoplace_uri),\n",
    "            (epistemic_location_uri, p.is_location_of, actor_uri),\n",
    "        ]\n",
    "\n",
    "    actors_uris[actor_label] = actor_uri\n",
    "    \n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca17d63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Inserting triples - 0.492 iterations in 00h00m00s (2.6 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Import persons with addresses\n",
    "\n",
    "triples = []\n",
    "persons_uris = {}\n",
    "eta.begin(len(all_persons), 'Importing persons')\n",
    "for person, addresses in all_persons.items():\n",
    "\n",
    "    # Create person\n",
    "    person_uri = create_uri()\n",
    "    person_label = person\n",
    "    triples += populate_entity(person_uri, c.person, person_label)\n",
    "    \n",
    "    # Link to all Geographical place\n",
    "    for address in addresses:\n",
    "        if address == '': continue\n",
    "        geoplace_uri = addresses_uris[address]\n",
    "\n",
    "        epistemic_location_uri = create_uri()\n",
    "        epistemic_location_label = f\"{address} - {person_label}\"\n",
    "        triples += populate_entity(epistemic_location_uri, c.epistemic_location_of_a_physical_thing, epistemic_location_label)\n",
    "        triples += [\n",
    "            (epistemic_location_uri, p.is_localized_at, geoplace_uri),\n",
    "            (epistemic_location_uri, p.is_location_of, person_uri),\n",
    "            (epistemic_location_uri, p.has_intentional_expression_identifing_type, k.epistemicLocationType_residence)\n",
    "        ]\n",
    "\n",
    "    persons_uris[person_label] = person_uri\n",
    "    \n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f038222d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Inserting triples - 0.013 iterations in 00h00m00s (0.1 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Import printers\n",
    "\n",
    "# List all printers\n",
    "printers = catalogs[['catalog_printer_name']].drop_duplicates()\n",
    "\n",
    "triples = []\n",
    "eta.begin(len(printers), 'Creating printers')\n",
    "for i, printer in printers.iterrows():\n",
    "    uri = create_uri()\n",
    "    label = printer['catalog_printer_name']\n",
    "    triples += populate_entity(uri, c.actor, label)\n",
    "\n",
    "    printers.at[i, \"printer_uri\"] = uri\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()\n",
    "\n",
    "\n",
    "# Add to catalog table\n",
    "catalogs = catalogs.merge(printers, on=\"catalog_printer_name\", how=\"left\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f3b3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating catalog owners - 5 iterations in 00h00m00s (624.8 iter/sec)                                     \n",
      "[ETA] Inserting triples - 0.01 iterations in 00h00m00s (0.1 iter/sec)                                          \n"
     ]
    }
   ],
   "source": [
    "# Imports catalog owners\n",
    "\n",
    "# List all printers\n",
    "catalog_owners = catalogs[['catalog_owner']].dropna().drop_duplicates()\n",
    "\n",
    "triples = []\n",
    "eta.begin(len(catalog_owners), 'Creating catalog owners')\n",
    "for i, owner in catalog_owners.iterrows():\n",
    "    uri = create_uri()\n",
    "    label = owner['catalog_owner']\n",
    "    triples += populate_entity(uri, c.actor, label)\n",
    "\n",
    "    catalog_owners.at[i, \"owner_uri\"] = uri\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()\n",
    "\n",
    "# Add to catalog table\n",
    "catalogs = catalogs.merge(catalog_owners, on=\"catalog_owner\", how=\"left\").drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b201cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating occupations - 5 iterations in 00h00m00s (738.7 iter/sec)                                        \n",
      "[ETA] Inserting triples - 0.01 iterations in 00h00m00s (0.1 iter/sec)                                          \n"
     ]
    }
   ],
   "source": [
    "# Import occupations\n",
    "occupations = catalogs[['seller_occupation']].dropna().drop_duplicates()\n",
    "\n",
    "triples = []\n",
    "occupations_uris = {}\n",
    "eta.begin(len(occupations), 'Creating occupations')\n",
    "for i, row in occupations.iterrows():\n",
    "    occupation_uri = create_uri()\n",
    "    occupation_label = row['seller_occupation']\n",
    "    triples += populate_entity(occupation_uri, c.occupation_peit, occupation_label)\n",
    "\n",
    "    occupations_uris[row['seller_occupation']] = occupation_uri\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2edceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating triples - 12596 iterations in 00h00m00s (16627.2 iter/sec)                                      \n",
      "[ETA] Inserting triples - 2.516 iterations in 00h00m00s (4.0 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Import Catalogs\n",
    "\n",
    "catalogs['lugt'] = catalogs['lugt'].astype(pd.Int64Dtype())\n",
    "\n",
    "# Create triples\n",
    "triples = []\n",
    "eta.begin(len(lots), 'Creating triples')\n",
    "for i, row in catalogs.iterrows():\n",
    "\n",
    "    # Information needed for the full row\n",
    "    date = row['name'][0:row['name'].index('_')]\n",
    "    catalog_name = f\"{row['name'][row['name'].index('_') + 1:].replace('-', ' ')} ({date})\"\n",
    "\n",
    "    # Physical Human made thing (the catalog)\n",
    "    phy_human_made_thing_uri = create_uri()\n",
    "    label = row['catalog_full_name']\n",
    "    triples += populate_entity(phy_human_made_thing_uri, c.physical_human_made_thing, label)\n",
    "\n",
    "    # Printer branch\n",
    "    if pd.notna(row['catalog_printer_name']):\n",
    "        production_uri = create_uri()\n",
    "        label = f\"Imprimé par {row['catalog_printer_name']}\"\n",
    "        triples += populate_entity(production_uri, c.production, label)\n",
    "\n",
    "        triples += [\n",
    "            (production_uri, p.has_produced, phy_human_made_thing_uri),\n",
    "            (production_uri, p.carried_out_by, row['printer_uri'])\n",
    "        ]\n",
    "\n",
    "    # URL (note) branch\n",
    "    if pd.notna(row['url']):\n",
    "        identifier_uri = create_uri()\n",
    "        identifier_label = \"Lien Gallica\"\n",
    "        triples += populate_entity(identifier_uri, c.identifier, identifier_label)\n",
    "        triples += [\n",
    "            (phy_human_made_thing_uri, p.same_as_external_identifier, identifier_uri),\n",
    "            (identifier_uri, p.has_identifier_type, k.identifierType_url),\n",
    "            (identifier_uri, p.is_identified_by, row['url']),\n",
    "        ]\n",
    "\n",
    "    # Lugt branch\n",
    "    if pd.notna(row['lugt']):\n",
    "        # Identifier\n",
    "        identifier_uri = create_uri()\n",
    "        identifier_label = f\"Lugt {row['lugt']}\"\n",
    "        triples += populate_entity(identifier_uri, c.identifier, identifier_label)\n",
    "\n",
    "        # Triples\n",
    "        triples += [\n",
    "            (phy_human_made_thing_uri, p.same_as_external_identifier, identifier_uri),\n",
    "            (identifier_uri, p.has_identifier_type, k.identifierType_lugt),\n",
    "            (identifier_uri, p.is_identified_by, to_literal(str(row['lugt'])))\n",
    "        ]\n",
    "\n",
    "    # Belonging branch\n",
    "    if pd.notna(row['catalog_owner']):\n",
    "        holding_a_right_or_obligation_uri = create_uri()\n",
    "        holding_a_right_or_obligation_label = f\"Appartenance du catalogue à {row['catalog_owner']}\"\n",
    "        triples += populate_entity(holding_a_right_or_obligation_uri, c.holding_a_right_or_obligation, holding_a_right_or_obligation_label) \n",
    "        triples += [\n",
    "            (holding_a_right_or_obligation_uri, p.is_subjection_of, phy_human_made_thing_uri),\n",
    "            (holding_a_right_or_obligation_uri, p.is_right_of, row['owner_uri']),\n",
    "        ]\n",
    "\n",
    "    # Distribution places branch\n",
    "    if pd.notna(row['catalog_distributed_places']):\n",
    "        distribution_places = row['catalog_distributed_places'].split('\\n')\n",
    "        for distributed_place in distribution_places:\n",
    "            address = distributed_place[:distributed_place.rindex(' - ')]\n",
    "            geoplace_uri = addresses_uris[address]\n",
    "\n",
    "            epistemic_location_uri = create_uri()\n",
    "            epistemic_location_label = f\"{catalog_name} distribué à {address}\"\n",
    "            triples += populate_entity(epistemic_location_uri, c.epistemic_location_of_a_physical_thing, epistemic_location_label)\n",
    "            triples += [\n",
    "                (epistemic_location_uri, p.is_location_of, phy_human_made_thing_uri),\n",
    "                (epistemic_location_uri, p.is_localized_at, geoplace_uri),\n",
    "                (epistemic_location_uri, p.has_location_type, k.epistemicLocationType_distributionPlace),\n",
    "            ]\n",
    "\n",
    "    # Expression branch\n",
    "    expression_uri = create_uri()\n",
    "    expression_label = f\"Contenu du catalogue {catalog_name}\"\n",
    "    triples += populate_entity(expression_uri, c.expression, expression_label)\n",
    "    triples += [(phy_human_made_thing_uri, p.carries, expression_uri)]\n",
    "\n",
    "    # Page number Branch\n",
    "    if pd.notna(row['catalog_page_number']):\n",
    "        page_number = str(int(float(row['catalog_page_number'])))\n",
    "        quantiQual_uri = create_uri()\n",
    "        quantiQual_label = f\"{page_number} pages\"\n",
    "        numDim_uri = create_uri()\n",
    "        numDim_label = f\"{page_number}\"\n",
    "        triples += populate_entity(quantiQual_uri, c.quantifiable_quality, quantiQual_label)\n",
    "        triples += populate_entity(numDim_uri, c.numeric_dimension, numDim_label)\n",
    "        triples += [\n",
    "            (expression_uri, p.has_quantifiable_quality, quantiQual_uri),\n",
    "            (quantiQual_uri, p.has_quality_type, k.entityQualityType_pageNumber),\n",
    "            (quantiQual_uri, p.has_quality_dimension, numDim_uri),\n",
    "            (numDim_uri, p.has_value, to_literal(page_number)),\n",
    "        ]\n",
    "\n",
    "    # Lot number Branch\n",
    "    if pd.notna(row['sale_lot_number']):\n",
    "        lot_number = str(int(float(row['sale_lot_number'])))\n",
    "        quantiQual_uri = create_uri()\n",
    "        quantiQual_label = f\"{lot_number} lots\"\n",
    "        numDim_uri = create_uri()\n",
    "        numDim_label = f\"{lot_number}\"\n",
    "        triples += populate_entity(quantiQual_uri, c.quantifiable_quality, quantiQual_label)\n",
    "        triples += populate_entity(numDim_uri, c.numeric_dimension, numDim_label)\n",
    "        triples += [\n",
    "            (expression_uri, p.has_quantifiable_quality, quantiQual_uri),\n",
    "            (quantiQual_uri, p.has_quality_type, k.entityQualityType_lotNumber),\n",
    "            (quantiQual_uri, p.has_quality_dimension, numDim_uri),\n",
    "            (numDim_uri, p.has_value, to_literal(lot_number))\n",
    "        ]\n",
    "\n",
    "    # Auctions branch\n",
    "    auction_uri = create_uri()\n",
    "    auction_label = catalog_name\n",
    "    triples += populate_entity(auction_uri, c.activity, auction_label)\n",
    "\n",
    "    # Auction Mentioning\n",
    "    mentioning_uri = create_uri()\n",
    "    mentioning_label = \"Mention de la vente aux enchères\"\n",
    "    triples += populate_entity(mentioning_uri, c.mentioning, mentioning_label)\n",
    "    triples += [\n",
    "        (mentioning_uri, p.mentions, auction_uri),\n",
    "        (mentioning_uri, p.is_mentioned_in, expression_uri)\n",
    "    ]\n",
    "\n",
    "    # Auction fees\n",
    "    if pd.notna(row['sale_fees']):\n",
    "        fees = float(row['sale_fees']) * 100\n",
    "        quantiQual_uri = create_uri()\n",
    "        quantiQual_label = f\"{fees}% de frais\"\n",
    "        triples += populate_entity(quantiQual_uri, c.quantifiable_quality_of_a_spatio_temporal_phenomenon, quantiQual_label)\n",
    "        numDim_uri = create_uri()\n",
    "        numDim_label = f\"{fees}%\"\n",
    "        triples += populate_entity(numDim_uri, c.numeric_dimension, numDim_label)\n",
    "        triples += [\n",
    "            (auction_uri, p.has_quantifiable_quality, quantiQual_uri),\n",
    "            (quantiQual_uri, p.has_quality_type, k.quantiQualSpati_fees),\n",
    "            (quantiQual_uri, p.has_quality_dimension, numDim_uri),\n",
    "            (numDim_uri, p.has_value, to_literal(str(fees)))\n",
    "        ]\n",
    "\n",
    "    # Auction Auctioneer\n",
    "    if pd.notna(row['auctioneer_names']):\n",
    "        for auctioneer in row['auctioneer_names'].split('\\n'):\n",
    "            person_uri = persons_uris[auctioneer]\n",
    "\n",
    "            participation_uri = create_uri()\n",
    "            participation_label = \"Commissaire priseur de la vente\"\n",
    "            triples += populate_entity(participation_uri, c.participation, participation_label)\n",
    "            triples+= [\n",
    "                (participation_uri, p.is_participation_in, auction_uri),\n",
    "                (participation_uri, p.is_participation_of, person_uri),\n",
    "                (participation_uri, p.is_participation_in_the_quality_of, k.actorSocialQuality_auctioneer)\n",
    "            ]\n",
    "\n",
    "    # Auction Expert\n",
    "    if pd.notna(row['experts_names']):\n",
    "        for expert in row['experts_names'].split('\\n'):\n",
    "            if expert == '': continue\n",
    "            person_uri = persons_uris[expert]\n",
    "\n",
    "            participation_uri = create_uri()\n",
    "            participation_label = \"Expert de la vente\"\n",
    "            triples += populate_entity(participation_uri, c.participation, participation_label)\n",
    "            triples+= [\n",
    "                (participation_uri, p.is_participation_in, auction_uri),\n",
    "                (participation_uri, p.is_participation_of, person_uri),\n",
    "                (participation_uri, p.is_participation_in_the_quality_of, k.actorSocialQuality_expert)\n",
    "            ]\n",
    "\n",
    "    # Auction Seller\n",
    "    if pd.notna(row['seller_name']):\n",
    "        for seller in row['seller_name'].split('\\n'):\n",
    "            person_uri = persons_uris[seller]\n",
    "\n",
    "            participation_uri = create_uri()\n",
    "            participation_label = \"Vendeur de la vente\"\n",
    "            triples += populate_entity(participation_uri, c.participation, participation_label)\n",
    "            triples+= [\n",
    "                (participation_uri, p.is_participation_in, auction_uri),\n",
    "                (participation_uri, p.is_participation_of, person_uri),\n",
    "                (participation_uri, p.is_participation_in_the_quality_of, k.actorSocialQuality_expert)\n",
    "            ]\n",
    "\n",
    "    # Auction Seller occupation\n",
    "    if pd.notna(row['seller_occupation']):\n",
    "        person_uri = persons_uris[row['seller_name']]\n",
    "        occupation_uri = occupations_uris[row['seller_occupation']]\n",
    "\n",
    "        occupation_teen_uri = create_uri()\n",
    "        occupation_teen_label = f\"{row['seller_name']} était {row['seller_occupation']}\"\n",
    "        triples += populate_entity(occupation_teen_uri, c.occupation_teen, occupation_teen_label)\n",
    "        triples += [\n",
    "            (occupation_teen_uri, p.is_occupation_of, person_uri),\n",
    "            (occupation_teen_uri, p.is_about, occupation_uri)\n",
    "        ]\n",
    "    \n",
    "    # Auction place\n",
    "    if pd.notna(row['sale_place']):\n",
    "        for sale_place in row['sale_place'].split('\\n'):\n",
    "            geoplace_uri = saleplaces_uris[sale_place]\n",
    "            triples+= [(auction_uri, p.took_place_at, geoplace_uri)]\n",
    "\n",
    "    # Activity Type \n",
    "    triples += [(auction_uri, p.has_activity_type, k.activityType_auction)]\n",
    "\n",
    "    # Dates\n",
    "    if pd.notna(row['sale_date_begin']):\n",
    "        triples += [(auction_uri, p.begin_of_the_begin, to_literal(row['sale_date_begin']))]\n",
    "    if pd.notna(row['sale_date_end']):\n",
    "        triples += [(auction_uri, p.end_of_the_end, to_literal(row['sale_date_end']))]\n",
    "\n",
    "    catalogs.at[i, 'catalog_uri'] = expression_uri\n",
    "    catalogs.at[i, 'auction_uri'] = auction_uri\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()\n",
    "\n",
    "# Save URIs for later access\n",
    "expression_uris = {}\n",
    "for i, row in catalogs[['name', 'catalog_uri']].iterrows():\n",
    "    expression_uris[row['name']] = row['catalog_uri']\n",
    "auctions_uris = {}\n",
    "for i, row in catalogs[['name', 'auction_uri']].iterrows():\n",
    "    auctions_uris[row['name']] = row['auction_uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a11a76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Creating lots - 12596 iterations in 00h02m18s (91.1 iter/sec)                                            \n",
      "[ETA] Inserting triples - 442.546 iterations in 00h01m31s (4.9 iter/sec)                                       \n"
     ]
    }
   ],
   "source": [
    "# Import Lots\n",
    "\n",
    "# Create triples\n",
    "triples = []\n",
    "eta.begin(len(lots), 'Creating lots')\n",
    "for i, row in lots.iterrows():\n",
    "\n",
    "    # Information needed for the full row\n",
    "    date = row['catalog'][0:row['catalog'].index('_')]\n",
    "    catalog_name = f\"{row['catalog'][row['catalog'].index('_') + 1:].replace('-', ' ')} ({date})\"\n",
    "    index = row['index']\n",
    "\n",
    "    # Physical Set (the lot)\n",
    "    physical_set_uri = create_uri()\n",
    "    label = f\"{catalog_name}, lot n°{index}\"\n",
    "    triples += populate_entity(physical_set_uri, c.physical_set, label)\n",
    "\n",
    "    # Identification branch\n",
    "    if pd.notna(row['index']):\n",
    "        triples.append((physical_set_uri, p.is_identified_by, to_literal(str(row['index']))))\n",
    "\n",
    "    # Description branch\n",
    "    if pd.notna(row['description']):\n",
    "        # Linguistic Object (lot description)\n",
    "        linguistic_object_uri = create_uri()\n",
    "        label = f\"Description du lot n°{str(index).upper()}, vente {catalog_name}\"\n",
    "        triples += populate_entity(linguistic_object_uri, c.linguistic_object, label)\n",
    "\n",
    "        triples += [\n",
    "            (linguistic_object_uri, p.is_about, physical_set_uri),\n",
    "            (linguistic_object_uri, p.has_language, k.language_french),\n",
    "            (linguistic_object_uri, p.has_symbolic_content, to_literal(row['description']))\n",
    "        ]\n",
    "\n",
    "    # Quantity branch\n",
    "    if pd.notna(row['number']):\n",
    "        # Quantifiable Quality  (items number)\n",
    "        quantifiable_quality_uri = create_uri()\n",
    "        label = f\"Nombre d'objet : {row['number']}\"\n",
    "        triples += populate_entity(quantifiable_quality_uri, c.quantifiable_quality, label)\n",
    "        # Numeric dimension (items number)\n",
    "        num_dimention_uri = create_uri()\n",
    "        label = str(row['number'])\n",
    "        triples += populate_entity(num_dimention_uri, c.numeric_dimension, label)\n",
    "\n",
    "        # Graph\n",
    "        triples += [\n",
    "            (physical_set_uri, p.has_quantifiable_quality, quantifiable_quality_uri),\n",
    "            (quantifiable_quality_uri, p.has_quality_type, k.entityQualityType_number),\n",
    "            (quantifiable_quality_uri, p.has_quality_dimension, num_dimention_uri),\n",
    "            (num_dimention_uri, p.has_value, to_literal(str(row['number'])))\n",
    "        ]\n",
    "\n",
    "    # Object type branch \n",
    "    if pd.notna(row['object_type']):\n",
    "        types = row['object_type'].split(', ')\n",
    "        triples += [(physical_set_uri, p.was_or_is_composed_of_object_of_type, object_types_dict[type]['uri']) for type in types]\n",
    "\n",
    "    # Material & Technique branch\n",
    "    if pd.notna(row['material_technique']):\n",
    "        mat_techs = row['material_technique'].split(', ')\n",
    "        triples += [(physical_set_uri, p.was_or_is_composed_of_objects_produced_with, material_techniques_dict[mat_tech]['uri']) for mat_tech in mat_techs]\n",
    "\n",
    "    # Production branch\n",
    "    if pd.notna(row['origin']) or pd.notna(row['author']) or pd.notna(row['period']):\n",
    "        # Production (fabrication)\n",
    "        production_uri = create_uri()\n",
    "        label = f\"Conception du lot n°{str(index).upper()} de {catalog_name}\"\n",
    "        triples += populate_entity(production_uri, c.production, label)\n",
    "        triples += [(production_uri, p.has_produced, physical_set_uri)]\n",
    "        if pd.notna(row['origin']):\n",
    "            triples += [(production_uri, p.took_place_at, origins_dict[origin]['uri']) for origin in row['origin'].split(', ')]\n",
    "        if pd.notna(row['author']):\n",
    "            triples += [(production_uri, p.carried_out_by, authors_dict[author]['uri']) for author in row['author'].split(', ')]\n",
    "        if pd.notna(row['period']):\n",
    "            triples += [(production_uri, p.has_time_span, periods_dict[period]['uri']) for period in row['period'].split(', ')]  \n",
    "    \n",
    "    # Mentioning (expression) Branch\n",
    "    mentioning_uri = create_uri()\n",
    "    mentioning_label = f\"Mention du lot n°{index}\"\n",
    "    triples += populate_entity(mentioning_uri, c.mentioning, mentioning_label)\n",
    "    catalog_uri = expression_uris[row['catalog']]\n",
    "    triples += [\n",
    "        (mentioning_uri, p.mentions, physical_set_uri),\n",
    "        (mentioning_uri, p.is_mentioned_in, catalog_uri)\n",
    "    ]\n",
    "\n",
    "    # Lot offered Branch\n",
    "    intentExpr_uri = create_uri()\n",
    "    intentExpr_label = f\"Mise en vente du lot n°{index}\"\n",
    "    triples += populate_entity(intentExpr_uri, c.intentional_expression, intentExpr_label)\n",
    "    # Offering of the lot\n",
    "    triples += [(intentExpr_uri, p.occured_in_the_presence_of, physical_set_uri)]\n",
    "    # Part of the auction\n",
    "    auction_uri = auctions_uris[row['catalog']]\n",
    "    triples += [(intentExpr_uri, p.is_part_of, auction_uri)]\n",
    "    # Mentioned in the catalog expression\n",
    "    expression_uri = expression_uris[row['catalog']]\n",
    "    mentioning_uri = create_uri()\n",
    "    mentioning_label = f\"Mention de la mise en vente du lot n°{index}\"\n",
    "    triples += populate_entity(mentioning_uri, c.mentioning, mentioning_label)\n",
    "    triples += [\n",
    "        (mentioning_uri, p.mentions, intentExpr_uri),\n",
    "        (mentioning_uri, p.is_mentioned_in, expression_uri)\n",
    "    ]\n",
    "\n",
    "    # Economic Transaction Branch\n",
    "    if pd.notna(row['sale_price']) or pd.notna(row['sale_buyer']):\n",
    "        econTrans_uri = create_uri()\n",
    "        econTrans_label = f\"Vente du lot n°{index}\"\n",
    "        triples += populate_entity(econTrans_uri, c.economic_transaction, econTrans_label)\n",
    "        triples += [\n",
    "            (econTrans_uri, p.occured_in_the_presence_of, physical_set_uri),\n",
    "            (econTrans_uri, p.has_setting, intentExpr_uri),\n",
    "        ]\n",
    "\n",
    "        if pd.notna(row['sale_price']):\n",
    "            sale_price = str(int(float(row['sale_price'])))\n",
    "            quantiQual_uri = create_uri()\n",
    "            quantiQual_label = f\"Adjugé à {sale_price} {row['sale_currency_label']}\"\n",
    "            triples += populate_entity(quantiQual_uri, c.quantifiable_quality_of_an_intentional_event, quantiQual_label)\n",
    "            monAmount_uri = create_uri()\n",
    "            monAmount_label = f\"{sale_price} {row['sale_currency_label']}\"\n",
    "            triples += populate_entity(monAmount_uri, c.monetary_amount, monAmount_label)\n",
    "            triples += [\n",
    "                (monAmount_uri, p.has_currency, row['sale_currency_uri']),\n",
    "                (monAmount_uri, p.has_value, to_literal(sale_price)),\n",
    "                (quantiQual_uri, p.has_quality_dimension, monAmount_uri),\n",
    "                (quantiQual_uri, p.has_quantifiable_quality_of_and_intentional_event_type, k.quantiQualIntEventType_hammerPrice),\n",
    "                (quantiQual_uri, p.is_quantifiable_quality_of, econTrans_uri)\n",
    "            ]\n",
    "\n",
    "        if pd.notna(row['sale_buyer']):\n",
    "            person_uri = persons_uris[row['sale_buyer']]\n",
    "            participation_uri = create_uri()\n",
    "            participation_label = \"Adjudicataire\"\n",
    "            triples += populate_entity(participation_uri, c.participation, participation_label)\n",
    "            triples += [\n",
    "                (participation_uri, p.is_participation_in, econTrans_uri),\n",
    "                (participation_uri, p.is_participation_of, person_uri),\n",
    "                (participation_uri, p.is_participation_in_the_quality_of, k.actorSocialQuality_buyer)\n",
    "            ]\n",
    "\n",
    "    lots.at[i, 'physical_set_uri'] = physical_set_uri\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5847e75",
   "metadata": {},
   "source": [
    "# Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f21fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontology definition\n",
    "\n",
    "ontology = {\n",
    "    v: k.replace('_', ' ').title()\n",
    "    for cls in (Classes, Properties)\n",
    "    for k, v in cls.__dict__.items()\n",
    "    if not k.startswith(\"__\")\n",
    "}\n",
    "\n",
    "graph_begin = \"graph \" + data_named_graph + \" {\" if data_named_graph else \"\"\n",
    "graph_end = \"}\" if data_named_graph else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c7588f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Inserting triples - 0.083 iterations in 00h00m00s (0.7 iter/sec)                                         \n"
     ]
    }
   ],
   "source": [
    "# Add class and properties label for Geovistory web component\n",
    "\n",
    "triples = []\n",
    "for key, value in ontology.items():\n",
    "    triples.append((key, 'rdfs:label', to_literal(value, 'en')))\n",
    "\n",
    "\n",
    "# Insert triples\n",
    "eta.begin(len(triples) / 1000, 'Inserting triples')\n",
    "sparql.insert(triples, data_named_graph, eta_fct=eta.iter)\n",
    "eta.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "962744e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all used classes\n",
    "\n",
    "classes = sparql.run(f\"\"\"\n",
    "    select distinct ?cls\n",
    "    where {{ \n",
    "        {graph_begin}\n",
    "            ?s rdf:type ?cls .\n",
    "        {graph_end}\n",
    "    }}\n",
    "\"\"\")\n",
    "classes = list(map(lambda x: x['cls'], classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "910a7234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETA] Building SHACL - 41 iterations in 00h00m13s (3.0 iter/sec)                                               \n"
     ]
    }
   ],
   "source": [
    "# Build SHACL\n",
    "\n",
    "shacl = \"\"\n",
    "for prefix in prefixes:\n",
    "    shacl += prefix.to_turtle() + '\\n'\n",
    "\n",
    "shacl += '\\n'\n",
    "\n",
    "eta.begin(len(classes), \"Building SHACL\")\n",
    "for cls in classes:\n",
    "    if cls not in ontology: continue\n",
    "    if ontology[cls] == \"\": continue\n",
    "\n",
    "    cls_raw = cls\n",
    "    cls = cls.replace('ontome', 'geov')\n",
    "\n",
    "    # Shape Name\n",
    "    shape_name = cls.replace(':', '_') + '_shape'\n",
    "\n",
    "    # Properties (outgoing) - Instances\n",
    "    outgoing_props_instances = sparql.run(f\"\"\"\n",
    "        select distinct ?prop_outgoing ?range_cls where {{\n",
    "            {graph_begin}\n",
    "                ?cls rdf:type {cls_raw} .\n",
    "                ?cls ?prop_outgoing ?range_instance .\n",
    "                ?range_instance rdf:type ?range_cls .\n",
    "            {graph_end}                    \n",
    "        }}\n",
    "    \"\"\")\n",
    "\n",
    "    # Properties (outgoing) - Values\n",
    "    outgoing_props_values = sparql.run(f\"\"\"\n",
    "        select distinct ?prop_outgoing where {{\n",
    "            {graph_begin}\n",
    "                ?cls rdf:type {cls_raw} .\n",
    "                ?cls ?prop_outgoing ?range_instance .\n",
    "                filter(isLiteral(?range_instance)).\n",
    "            {graph_end}       \n",
    "        }}\n",
    "    \"\"\")\n",
    "\n",
    "    # Properties (incoming)\n",
    "    incoming_props = sparql.run(f\"\"\"\n",
    "        select distinct ?prop_incoming ?domain_cls where {{\n",
    "            {graph_begin}\n",
    "                ?cls rdf:type {cls_raw} .\n",
    "                ?domain_instance ?prop_incoming ?cls .\n",
    "                ?domain_instance rdf:type ?domain_cls .\n",
    "            {graph_end}       \n",
    "        }}\n",
    "    \"\"\")\n",
    "\n",
    "    shacl += '\\n'\n",
    "    shacl += f'sdh-shacl:{shape_name} a sh:NodeShape ;\\n'\n",
    "    shacl += f'    sh:targetClass {cls} ;\\n'\n",
    "    shacl += f'    sh:name \"{ontology[cls]}\" ;\\n'\n",
    "\n",
    "    for prop in outgoing_props_values:\n",
    "        prop_uri = prop['prop_outgoing']\n",
    "\n",
    "        if prop_uri not in ontology: continue\n",
    "        if ontology[prop_uri] == \"\": continue\n",
    "\n",
    "        shacl += '\\n'\n",
    "        shacl += f'    sh:property [\\n'\n",
    "        shacl += f'        sh:path {prop_uri} ;\\n'\n",
    "        shacl += f'        sh:name \"{ontology[prop_uri]}\" ;\\n'\n",
    "        shacl += f'        sh:datatype xsd:string ;\\n'\n",
    "        shacl += f'    ] ;\\n'\n",
    "\n",
    "    for prop in outgoing_props_instances:\n",
    "        prop_uri = prop['prop_outgoing'].replace('ontome', 'geov')\n",
    "\n",
    "        if prop_uri not in ontology: continue\n",
    "        if ontology[prop_uri] == \"\": continue\n",
    "\n",
    "        range_uri = prop['range_cls']\n",
    "        shacl += '\\n'\n",
    "        shacl += f'    sh:property [\\n'\n",
    "        shacl += f'        sh:path {prop_uri} ;\\n'\n",
    "        shacl += f'        sh:name \"{ontology[prop_uri]}\" ;\\n'\n",
    "        shacl += f'        sh:class {range_uri} ;\\n'\n",
    "        shacl += f'    ] ;\\n'\n",
    "\n",
    "    for prop in incoming_props:\n",
    "        prop_uri = prop['prop_incoming'].replace('ontome', 'geov')\n",
    "\n",
    "        if prop_uri not in ontology: continue\n",
    "        if ontology[prop_uri] == \"\": continue\n",
    "        \n",
    "        domain_uri = prop['domain_cls']\n",
    "        shacl += '\\n'\n",
    "        shacl += f'    sh:property [\\n'\n",
    "        shacl += f'        sh:path [\\n'\n",
    "        shacl += f'            sh:inversePath {prop_uri} ;\\n'\n",
    "        shacl += f'        ] ;\\n'\n",
    "        shacl += f'        sh:name \"{ontology[prop_uri]}\" ;\\n'\n",
    "        shacl += f'    ] ;\\n'\n",
    "\n",
    "\n",
    "    shacl += f'\\n    .\\n'\n",
    "\n",
    "    eta.iter()\n",
    "eta.end()\n",
    "\n",
    "file = open('./objective-shacl.ttl', 'w')\n",
    "file.write(shacl)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c81fa104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Uploading 10000 (0 / 1116) Done\n"
     ]
    }
   ],
   "source": [
    "# Upload SHACL content into the endpoint\n",
    "\n",
    "sparql.upload_turtle(shacl, shacl_named_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd81b1c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7136e7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at: 10:02:15\n",
      "Elapsed time: 0h 5m 9s\n"
     ]
    }
   ],
   "source": [
    "# Finished time and Elapsed calculation\n",
    "\n",
    "end = datetime.now()\n",
    "print('Finished at:', end.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "elapsed = end - begin\n",
    "seconds = int(elapsed.total_seconds())\n",
    "hours, remainder = divmod(seconds, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(f\"Elapsed time: {hours}h {minutes}m {seconds}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
